{
  "hash": "87947e0f65bf370e71bb1d22c6226593",
  "result": {
    "markdown": "---\ntitle: \"Replication exercise for matching methods\"\nauthor: \"Katherine Siegel\"\ndate: \"2023-02-21\"\noutput: html_document\n---\n\n\n## Description\nIn this replication exercise, you will use some of the data from Siegel et al. 2022 (https://doi.org/10.1007/s10113-022-01950-y). The dataset for the entire western US is very large and unwieldy, so you'll work with data from a single state: Colorado. Note: all the code is commented out in this Rmd -- you'll need to uncomment the parts you want to use.\n\n## Set up\n\n\n\n\n## The data\nThere are two data files you can play around with for this exercise: *colo_dat_full.csv* and *colo_data_for_matching.csv*. *colo_dat_full.csv* has the entire time series of data for the full (unmatched) dataset of federal and private forests in Colorado. *colo_data_for_matching.csv* is a file that's ready for the matching process without additional pre-processing: it has a row for each sample point in Colorado and five-year averages for the climate variables. \n\n### Variable names in colo_dat_full  \n\n* state: the state the sample is from (it should always be Colorado in this file)  \n* UID: a unique identifier for each sample point  \n* year: the year that the fire and climate data is from  \n* burned: whether or not the site burned in that year (0 = unburned, 1 = burned)  \n* prot_cat_recl: the ownership class. 0 = private, 1 = federal  \n* dist_rds_km: distance to the nearest road, in km  \n* slope: slope, in degrees  \n* aspect_srai: aspect  \n* elev_km: elevation, in 1000 m  \n* lon: longitude  \n* lat: latitude  \n* lightning: county-level lightning strikes    \n* pdsi_avg_season: seasonal average Palmer Drought Severity Index value  \n* soil_avg_season: seasonal average soil moisture  \n* tmmn_avg_season: seasonal average minimum temperature\n* tmmx_avg_season: seasonal average maximum temperature  \n* vs_max_season: seasonal average maximum wind speed  \n* total_precip_season: total seasonal precipitation    \n* prev_yr_precip: total precipitation in the previous year\n\n### Variable names in colo_data_for_matching  \n\nAll the climate variables are variablename_5 to indicate that they are 5-year average values\n* UID: a unique identifier for each sample point  \n* state: the state the sample is from  \n* prot_cat_recl: the ownership class. 0 = private, 1 = federal  \n* lightning_5: 5 year average for lightning strikes  \n* vs_max_season: seasonal average maximum wind speed  \n* pr_total_season: total seasonal precipitation    \n* tmmx_avg_season: seasonal average maximum temperature  \n* tmmn_avg_season: seasonal average minimum temperature\n* pdsi_avg_season: seasonal average Palmer Drought Severity Index value  \n* soil_avg_season: seasonal average soil moisture  \n* slope: slope, in degrees  \n* aspect_srai: aspect  \n* elev_km: elevation, in 1000 m  \n* lon: longitude  \n* lat: latitude  \n* dist_rds_km: distance to the nearest road, in km  \n* popdens_1990: population density (per km2) in 1990  \n* popdens_2000: population density (per km2) in 2000  \n* popdens_2010: population density (per km2) in 2010  \n\n\n## Prep the data\n\n::: {.cell}\n\n:::\n\n\n## Check out the full, unmatched dataset\nYou might want to make the prot_cat_recl variable a factor.\n\n::: {.cell}\n\n```{.r .cell-code}\n### See how many treated (federal) and control (private) sample points we have\n\n\n### You can take a look at the balance of the covariates before matching via data visualizations (boxplots, histograms, density plots)\n\n\n### You can also look at it in table form\n```\n:::\n\n\n## Match the data  \nMatch the data on the observed covariates, using the MatchIt package. You can play around with the settings to see how it affects the matched data you end up with. \n\n::: {.cell}\n\n```{.r .cell-code}\n### Match the data \n```\n:::\n\n\n#### Assess match quality\nTake a look at the quality of the matches: how many points were matched? what was the pre-matching covariate balance? what was the covariate balance after matching?\n\n::: {.cell}\n\n:::\n\n\n#### Visualize the match quality\n\n::: {.cell}\n\n```{.r .cell-code}\n### Some easy visualizations through MatchiIt\n# plot(match, interactive = FALSE)\n# plot(match, type = \"jitter\", interactive = FALSE)\n```\n:::\n\n\n#### Iterate, if necessary\nIf you're satisfied with the quality of your matches, you can move on to the analysis. Otherwise, try tweaking the parameters to see what happens.\n\n::: {.cell}\n\n:::\n\n\n## Analyze the matched dataset\n### Extract the matched data\nFirst, you'll need to extract the matched data and use the UIDs from the matched data to subset the full dataset for analysis. Here is some code to demonstrate that (you'll likely need to modify this)\n\n::: {.cell}\n\n```{.r .cell-code}\n# ### Extract the matches from the MatchIt object\n# matched <- match.data(match)\n# \n# ### Open the full data (entire time series) \n# full_data <- read.csv(\"file_path/colo_dat_full.csv\")\n# \n# ### Filter full_data to just include the UIDs of the matched subset of the data\n# full_data <- full_data %>%\n#   filter(UID %in% matched$UID)\n```\n:::\n\n\n### Add time lag variables\nIn my analyses, I used time lag variables to account for recent fire history: if a site burned last year, it usually won't burn this year. So we'll make a variable that accounts for whether or not a site burned in the previous five years. \n\nSince we used a lag variable, we won't have complete data for the first few years of the time series, so we'll also filter the dataset to just include the years with complete data.\n\n::: {.cell}\n\n```{.r .cell-code}\n# ### Add lag variable\n# full_data <- full_data %>%\n#   \n#   ### Group by the UID so you can make a site-specific lag variable\n#   group_by(UID) %>%\n#   \n#   ### Add the lag variable\n#   mutate(burn_prev_5_yr = dplyr::lag(burned, \n#                                      n = 5, \n#                                      default = NA,\n#                                      order_by = year)) %>%\n#   ungroup()\n# \n# ### Drop first years of time series\n# full_data <- full_data %>%\n#   filter(year > 1988)\n```\n:::\n\n\n### Check for correlated variables\nSome of the continuous variables are highly correlated, so we don't want to include them in the regression model.\n\n::: {.cell}\n\n```{.r .cell-code}\n### Assess level of correlation\n\n### if you want to use it, here's a function that returns a table with the correlation coefficient and p-value between each pair of variables:\n# cor.prob <- function (X, dfr = nrow(X) - 2) {\n#   R <- cor(X, use=\"pairwise.complete.obs\")\n#   above <- row(R) < col(R)\n#   r2 <- R[above]^2\n#   Fstat <- r2 * dfr/(1 - r2)\n#   R[above] <- 1 - pf(Fstat, 1, dfr)\n#   R[row(R) == col(R)] <- NA\n#   R\n# }\n# flattenSquareMatrix <- function(m) {\n#   if( (class(m) != \"matrix\") | (nrow(m) != ncol(m))) stop(\"Must be a square matrix.\") \n#   if(!identical(rownames(m), colnames(m))) stop(\"Row and column names must be equal.\")\n#   ut <- upper.tri(m)\n#   data.frame(i = rownames(m)[row(m)[ut]],\n#              j = rownames(m)[col(m)[ut]],\n#              cor=t(m)[ut],\n#              p=m[ut])\n# }\n\n### Determine which variables to drop\n```\n:::\n\n\n### Model the effect of ownership/management on wildfire probability for a given site in a given year\nThis is the exciting part! Here's a basic logistic regression model, but feel free to tweak it.\n\n::: {.cell}\n\n```{.r .cell-code}\n# ### Model\n# colo_model <- glmer(burned ~ # covariates you're including + \n#                       \n#                       ### include an interaction between year and ownership \n#                       ### don't forget to tell R that prot_cat_recl is a factor\n#                       prot_cat_recl*year + \n#                       \n#                       ### add the lag variable\n#                       burn_prev_5_yr +\n#                       \n#                       ### and a site-level effect to account for unobservable \n#                       ### factors unique to each site\n#                       (1|UID),\n#                     \n#                     ### tell it where to get the data from\n#                     data = full_data, \n#                     \n#                     ### model parameters\n#                     family = binomial(link = \"logit\"),\n#                     nAGQ = 0,\n#                     control = glmerControl(optimizer = \"nloptwrap\"))\n\n### Extract the model's coefficient estimates\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}